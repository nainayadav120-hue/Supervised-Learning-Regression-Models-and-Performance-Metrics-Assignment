{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Learning: Regression Models and Performance Metrics Assignment"
      ],
      "metadata": {
        "id": "eJapns1-fRzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\"\"\" Simple Linear Regression (SLR) is a statistical method that models the relationship between a single independent variable (predictor) and a\n",
        "continuous dependent variable (target). The purpose of SLR is to:\n",
        "1. Describe the relationship: SLR helps understand how the independent variable affects the dependent variable.\n",
        "2. Make predictions: By estimating the relationship, SLR enables predicting the dependent variable's value based on the independent variable.\n",
        "3. Analyze the strength and direction: SLR provides insights into the strength and direction of the relationship between the variables.\n",
        "\n",
        "The SLR equation is: y = β0 + β1x + ε\n",
        "where:\n",
        "- y is the dependent variable\n",
        "- x is the independent variable\n",
        "- β0 is the intercept\n",
        "- β1 is the slope coefficient\n",
        "- ε is the error term\n",
        "SLR is widely used in various fields, such as economics, finance, and social sciences, to model and analyze relationships between variables.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mGIUkeXcfZeG",
        "outputId": "812ca268-2e7f-4afe-ad15-8de8154473ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Simple Linear Regression (SLR) is a statistical method that models the relationship between a single independent variable (predictor) and a \\ncontinuous dependent variable (target). The purpose of SLR is to:\\n1. Describe the relationship: SLR helps understand how the independent variable affects the dependent variable.\\n2. Make predictions: By estimating the relationship, SLR enables predicting the dependent variable's value based on the independent variable.\\n3. Analyze the strength and direction: SLR provides insights into the strength and direction of the relationship between the variables.\\n\\nThe SLR equation is: y = β0 + β1x + ε\\nwhere:\\n- y is the dependent variable\\n- x is the independent variable\\n- β0 is the intercept\\n- β1 is the slope coefficient\\n- ε is the error term\\nSLR is widely used in various fields, such as economics, finance, and social sciences, to model and analyze relationships between variables.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. What are the key assumptions of Simple Linear Regression?\n",
        "\"\"\"Key Assumptions of Simple Linear Regression\n",
        "1. Linearity: The relationship between the independent variable (x) and the dependent variable (y) is linear.\n",
        "2. Independence: Observations are independent of each other.\n",
        "3. Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
        "4. Normality: The residuals are normally distributed.\n",
        "5. No Multicollinearity: Not applicable in Simple Linear Regression (since there's only one independent variable), but in general,\n",
        "it means that independent variables should not be highly correlated with each other.\n",
        "Consequences of violating assumptions:\n",
        "- Violating these assumptions can lead to biased or inefficient estimates of the regression coefficients.\n",
        "- It can also affect the accuracy of predictions and the reliability of statistical inferences.\n",
        "\n",
        "Checking assumptions:\n",
        "- Linearity: Scatter plots\n",
        "- Independence: Durbin-Watson test\n",
        "- Homoscedasticity: Residual plots, Breusch-Pagan test\n",
        "- Normality: Q-Q plots, Shapiro-Wilk test\n",
        "By ensuring these assumptions are met, you can increase the validity and reliability of your Simple Linear Regression model.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "xyfUYNfGgPR-",
        "outputId": "236600cf-8c9b-4689-c8a2-f9cba0f81b6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Key Assumptions of Simple Linear Regression\\n1. Linearity: The relationship between the independent variable (x) and the dependent variable (y) is linear.\\n2. Independence: Observations are independent of each other.\\n3. Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\\n4. Normality: The residuals are normally distributed.\\n5. No Multicollinearity: Not applicable in Simple Linear Regression (since there's only one independent variable), but in general, \\nit means that independent variables should not be highly correlated with each other.\\nConsequences of violating assumptions:\\n- Violating these assumptions can lead to biased or inefficient estimates of the regression coefficients.\\n- It can also affect the accuracy of predictions and the reliability of statistical inferences.\\n\\nChecking assumptions:\\n- Linearity: Scatter plots\\n- Independence: Durbin-Watson test\\n- Homoscedasticity: Residual plots, Breusch-Pagan test\\n- Normality: Q-Q plots, Shapiro-Wilk test\\nBy ensuring these assumptions are met, you can increase the validity and reliability of your Simple Linear Regression model.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write the mathematical equation for a simple linear regression model and explain each term.\n",
        "\"\"\"The mathematical equation for a simple linear regression model is:\n",
        "y = β0 + β1x + ε\n",
        "\n",
        "Where:\n",
        "1. y: Dependent variable (target variable) - the variable being predicted or explained.\n",
        "2. x: Independent variable (predictor variable) - the variable used to predict or explain the dependent variable.\n",
        "3. β0 (Intercept): The expected value of y when x is 0. It represents the baseline value of y.\n",
        "4. β1 (Slope Coefficient): The change in y for a one-unit change in x, while holding all other factors constant. It represents the strength and direction of the relationship between x and y.\n",
        "5. ε (Error Term): The difference between the observed value of y and the predicted value. It represents the random variation in y that is not explained by x.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- β0: The value of y when x is 0.\n",
        "- β1: A positive value indicates a positive relationship between x and y, while a negative value indicates a negative relationship. The magnitude of β1 represents the strength of the relationship.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we have a simple linear regression model: y = 2 + 3x + ε\n",
        "\n",
        "- β0 = 2: The expected value of y is 2 when x is 0.\n",
        "- β1 = 3: For every one-unit increase in x, y is expected to increase by 3 units.\n",
        "\n",
        "By understanding the coefficients and the equation, you can make predictions and interpret the relationships between variables in a simple linear regression model.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "Fzt9mMoCg1tF",
        "outputId": "b4d5a3b4-dac4-423f-81e9-eea571ff3927"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The mathematical equation for a simple linear regression model is:\\ny = β0 + β1x + ε\\n\\nWhere:\\n1. y: Dependent variable (target variable) - the variable being predicted or explained.\\n2. x: Independent variable (predictor variable) - the variable used to predict or explain the dependent variable.\\n3. β0 (Intercept): The expected value of y when x is 0. It represents the baseline value of y.\\n4. β1 (Slope Coefficient): The change in y for a one-unit change in x, while holding all other factors constant. It represents the strength and direction of the relationship between x and y.\\n5. ε (Error Term): The difference between the observed value of y and the predicted value. It represents the random variation in y that is not explained by x.\\n\\nInterpretation:\\n\\n- β0: The value of y when x is 0.\\n- β1: A positive value indicates a positive relationship between x and y, while a negative value indicates a negative relationship. The magnitude of β1 represents the strength of the relationship.\\n\\nExample:\\n\\nSuppose we have a simple linear regression model: y = 2 + 3x + ε\\n\\n- β0 = 2: The expected value of y is 2 when x is 0.\\n- β1 = 3: For every one-unit increase in x, y is expected to increase by 3 units.\\n\\nBy understanding the coefficients and the equation, you can make predictions and interpret the relationships between variables in a simple linear regression model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Provide a real-world example where simple linear regression can be applied.\n",
        "#Data used in example:\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Square Footage': [1000, 1200, 1500, 1800, 2000],\n",
        "    'Price': [200000, 240000, 300000, 360000, 400000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "\"\"\"Real-World Example: Predicting House Prices Based on Square Footage\n",
        "\n",
        "Scenario: A real estate agent wants to predict the price of a house based on its square footage.\n",
        "The agent collects data on the square footage (in square feet) and the corresponding prices (in dollars) of several houses.\n",
        "Simple Linear Regression Model:\n",
        "The agent can use simple linear regression to model the relationship between square footage (x) and price (y). The equation might look like:\n",
        "Price = β0 + β1 × Square Footage + ε\n",
        "\n",
        "Interpretation:\n",
        "- β0: The base price of a house with 0 square footage (not meaningful in this context, but necessary for the model).\n",
        "- β1: The increase in price for each additional square foot of living space.\n",
        "\n",
        "Predictions:\n",
        "Using the model, the agent can predict the price of a new house based on its square footage.\n",
        "For example, if the model estimates β0 = 50,000 and β1 = 175, the predicted price for a 1600 square foot house would be:\n",
        "Price = 50,000 + 175 × 1600 = 330,000\n",
        "This example demonstrates how simple linear regression can be applied to predict house prices based on square footage,\n",
        "providing valuable insights for real estate agents and homeowners.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "GFvX-q8-g7lq",
        "outputId": "0980d4b2-5f9a-49fd-95a1-df8396a570df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Square Footage   Price\n",
            "0            1000  200000\n",
            "1            1200  240000\n",
            "2            1500  300000\n",
            "3            1800  360000\n",
            "4            2000  400000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Real-World Example: Predicting House Prices Based on Square Footage\\n\\nScenario: A real estate agent wants to predict the price of a house based on its square footage. \\nThe agent collects data on the square footage (in square feet) and the corresponding prices (in dollars) of several houses.\\nSimple Linear Regression Model:\\nThe agent can use simple linear regression to model the relationship between square footage (x) and price (y). The equation might look like:\\nPrice = β0 + β1 × Square Footage + ε\\n\\nInterpretation:\\n- β0: The base price of a house with 0 square footage (not meaningful in this context, but necessary for the model).\\n- β1: The increase in price for each additional square foot of living space.\\n\\nPredictions:\\nUsing the model, the agent can predict the price of a new house based on its square footage. \\nFor example, if the model estimates β0 = 50,000 and β1 = 175, the predicted price for a 1600 square foot house would be:\\nPrice = 50,000 + 175 × 1600 = 330,000\\nThis example demonstrates how simple linear regression can be applied to predict house prices based on square footage,\\nproviding valuable insights for real estate agents and homeowners.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. What is the method of least squares in linear regression?\n",
        "\"\"\"Method of Least Squares\n",
        "The method of least squares is a statistical technique used to determine the best-fitting line for a set of data points by minimizing the sum of the squared residuals. In linear regression, the goal is to find the optimal values of the coefficients (β0 and β1) that minimize the difference between the observed values and the predicted values.\n",
        "\n",
        "How it works:\n",
        "1. Residuals: Calculate the residuals (errors) between the observed values and the predicted values.\n",
        "2. Squared Residuals: Square each residual to give more weight to larger errors.\n",
        "3. Sum of Squared Residuals: Calculate the sum of the squared residuals.\n",
        "4. Minimization: Find the values of β0 and β1 that minimize the sum of the squared residuals.\n",
        "\n",
        "Mathematical Representation:\n",
        "The method of least squares minimizes the following equation:\n",
        "SSR (Sum of Squared Residuals) = Σ(yi - (β0 + β1xi))^2\n",
        "\n",
        "where:\n",
        "- yi is the observed value\n",
        "- xi is the independent variable\n",
        "- β0 and β1 are the coefficients to be estimated\n",
        "\n",
        "Advantages:\n",
        "1. Best Linear Unbiased Estimator (BLUE): The method of least squares provides the best linear unbiased estimator for the coefficients.\n",
        "2. Simple to Implement: The method is relatively simple to understand and implement.\n",
        "\n",
        "Common Applications:\n",
        "1. Linear Regression: The method of least squares is widely used in linear regression to model the relationship between a dependent variable and one or more independent variables.\n",
        "2. Data Analysis: The method is used in various fields, such as economics, finance, and engineering, to analyze and model relationships between variables.\n",
        "\n",
        "By minimizing the sum of the squared residuals, the method of least squares provides a robust and reliable way to estimate the coefficients in linear regression.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ncnspJUrg7yH",
        "outputId": "d94b32ae-eebc-4393-fc09-538c15628917"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Method of Least Squares\\nThe method of least squares is a statistical technique used to determine the best-fitting line for a set of data points by minimizing the sum of the squared residuals. In linear regression, the goal is to find the optimal values of the coefficients (β0 and β1) that minimize the difference between the observed values and the predicted values.\\n\\nHow it works:\\n1. Residuals: Calculate the residuals (errors) between the observed values and the predicted values.\\n2. Squared Residuals: Square each residual to give more weight to larger errors.\\n3. Sum of Squared Residuals: Calculate the sum of the squared residuals.\\n4. Minimization: Find the values of β0 and β1 that minimize the sum of the squared residuals.\\n\\nMathematical Representation:\\nThe method of least squares minimizes the following equation:\\nSSR (Sum of Squared Residuals) = Σ(yi - (β0 + β1xi))^2\\n\\nwhere:\\n- yi is the observed value\\n- xi is the independent variable\\n- β0 and β1 are the coefficients to be estimated\\n\\nAdvantages:\\n1. Best Linear Unbiased Estimator (BLUE): The method of least squares provides the best linear unbiased estimator for the coefficients.\\n2. Simple to Implement: The method is relatively simple to understand and implement.\\n\\nCommon Applications:\\n1. Linear Regression: The method of least squares is widely used in linear regression to model the relationship between a dependent variable and one or more independent variables.\\n2. Data Analysis: The method is used in various fields, such as economics, finance, and engineering, to analyze and model relationships between variables.\\n\\nBy minimizing the sum of the squared residuals, the method of least squares provides a robust and reliable way to estimate the coefficients in linear regression.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\"\"\" Logistic Regression\n",
        "Logistic regression is a type of supervised learning algorithm used for classification problems.\n",
        "It's a popular method for predicting the probability of a binary outcome (0 or 1, yes or no, etc.) based on one or more predictor variables.\n",
        "\n",
        "How it works:\n",
        "1. Sigmoid Function: Logistic regression uses the sigmoid function (also known as the logistic function) to model the probability of the positive outcome.\n",
        "2. Probability Prediction: The model predicts the probability of the positive outcome based on the input variables.\n",
        "3. Classification: The predicted probability is then used to classify the outcome as 0 or 1.\n",
        "\n",
        "Logistic Regression Equation:\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "where:\n",
        "- p is the predicted probability\n",
        "- e is the base of the natural logarithm\n",
        "- z is a linear combination of the input variables and coefficients\n",
        "\n",
        "Difference from Linear Regression:\n",
        "1. Outcome Variable: Logistic regression is used for binary classification problems, while linear regression is used for continuous outcome variables.\n",
        "2. Sigmoid Function: Logistic regression uses the sigmoid function to model the probability of the positive outcome, while linear regression uses a linear function to model the relationship between the variables.\n",
        "3. Output: Logistic regression outputs a probability between 0 and 1, while linear regression outputs a continuous value.\n",
        "When to use Logistic Regression:\n",
        "1. Binary Classification: Use logistic regression when the outcome variable is binary (0 or 1, yes or no, etc.).\n",
        "2. Probability Prediction: Use logistic regression when you want to predict the probability of a positive outcome.\n",
        "\n",
        "Common Applications:\n",
        "1. Credit Risk Assessment: Logistic regression is used to predict the probability of loan default based on credit score, income, and other factors.\n",
        "2. Medical Diagnosis: Logistic regression is used to predict the probability of a disease based on symptoms, test results, and other factors.\n",
        "3. Marketing: Logistic regression is used to predict the probability of a customer responding to a marketing campaign based on demographic and behavioral data.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "SuNaEFLKg8Ba",
        "outputId": "efeabdec-fe6d-4017-ca40-b0ececdee768"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Logistic Regression\\nLogistic regression is a type of supervised learning algorithm used for classification problems. \\nIt's a popular method for predicting the probability of a binary outcome (0 or 1, yes or no, etc.) based on one or more predictor variables.\\n\\nHow it works:\\n1. Sigmoid Function: Logistic regression uses the sigmoid function (also known as the logistic function) to model the probability of the positive outcome.\\n2. Probability Prediction: The model predicts the probability of the positive outcome based on the input variables.\\n3. Classification: The predicted probability is then used to classify the outcome as 0 or 1.\\n\\nLogistic Regression Equation:\\np = 1 / (1 + e^(-z))\\n\\nwhere:\\n- p is the predicted probability\\n- e is the base of the natural logarithm\\n- z is a linear combination of the input variables and coefficients\\n\\nDifference from Linear Regression:\\n1. Outcome Variable: Logistic regression is used for binary classification problems, while linear regression is used for continuous outcome variables.\\n2. Sigmoid Function: Logistic regression uses the sigmoid function to model the probability of the positive outcome, while linear regression uses a linear function to model the relationship between the variables.\\n3. Output: Logistic regression outputs a probability between 0 and 1, while linear regression outputs a continuous value.\\nWhen to use Logistic Regression:\\n1. Binary Classification: Use logistic regression when the outcome variable is binary (0 or 1, yes or no, etc.).\\n2. Probability Prediction: Use logistic regression when you want to predict the probability of a positive outcome.\\n\\nCommon Applications:\\n1. Credit Risk Assessment: Logistic regression is used to predict the probability of loan default based on credit score, income, and other factors.\\n2. Medical Diagnosis: Logistic regression is used to predict the probability of a disease based on symptoms, test results, and other factors.\\n3. Marketing: Logistic regression is used to predict the probability of a customer responding to a marketing campaign based on demographic and behavioral data.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Name and briefly describe three common evaluation metrics for regression models.\n",
        "\"\"\" Three common evaluation metrics for regression models:\n",
        "\n",
        "1. Mean Absolute Error (MAE): Measures the average difference between predicted and actual values. It gives equal weight to all errors.\n",
        "\n",
        "2. Mean Squared Error (MSE): Measures the average of the squared differences between predicted and actual values. It gives more weight to larger errors.\n",
        "\n",
        "3. R-Squared (R²): Measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It indicates the goodness of fit of the model, with values closer to 1 indicating a better fit.\n",
        "\n",
        "These metrics help assess the performance and accuracy of regression models.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "tvgEXWAQi7j5",
        "outputId": "f0f673ea-1e07-4f89-a0f2-9131a135c977"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Three common evaluation metrics for regression models:\\n\\n1. Mean Absolute Error (MAE): Measures the average difference between predicted and actual values. It gives equal weight to all errors.\\n\\n2. Mean Squared Error (MSE): Measures the average of the squared differences between predicted and actual values. It gives more weight to larger errors.\\n\\n3. R-Squared (R²): Measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It indicates the goodness of fit of the model, with values closer to 1 indicating a better fit.\\n\\nThese metrics help assess the performance and accuracy of regression models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. What is the purpose of the R-squared metric in regression analysis?\n",
        "\"\"\" R-squared (R²) Metric:\n",
        "The R-squared metric, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a regression model.\n",
        "\n",
        "Purpose:\n",
        "1. Goodness of Fit: R-squared indicates how well the model fits the data, with higher values indicating a better fit.\n",
        "2. Explained Variance: R-squared shows the proportion of the variance in the dependent variable that is explained by the independent variable(s).\n",
        "3. Model Evaluation: R-squared is used to evaluate the performance of a regression model and compare it to other models.\n",
        "\n",
        "Interpretation:\n",
        "- R² = 1: Perfect fit, all variance is explained by the model.\n",
        "- R² = 0: No relationship between the variables, model does not explain any variance.\n",
        "- R² between 0 and 1: Proportion of variance explained by the model.\n",
        "\n",
        "Limitations:\n",
        "- Does not indicate causality: R-squared does not imply causation between the variables.\n",
        "- Can be influenced by outliers: Outliers can affect R-squared values.\n",
        "- Does not account for model complexity: R-squared can increase with additional variables, regardless of their relevance.\n",
        "\n",
        "By using R-squared, you can gain insights into the performance and explanatory power of your regression model.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "AVftkKFKi7xH",
        "outputId": "fb72dd10-0791-4db3-e3c7-7df86799624f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' R-squared (R²) Metric:\\nThe R-squared metric, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s) in a regression model.\\n\\nPurpose:\\n1. Goodness of Fit: R-squared indicates how well the model fits the data, with higher values indicating a better fit.\\n2. Explained Variance: R-squared shows the proportion of the variance in the dependent variable that is explained by the independent variable(s).\\n3. Model Evaluation: R-squared is used to evaluate the performance of a regression model and compare it to other models.\\n\\nInterpretation:\\n- R² = 1: Perfect fit, all variance is explained by the model.\\n- R² = 0: No relationship between the variables, model does not explain any variance.\\n- R² between 0 and 1: Proportion of variance explained by the model.\\n\\nLimitations:\\n- Does not indicate causality: R-squared does not imply causation between the variables.\\n- Can be influenced by outliers: Outliers can affect R-squared values.\\n- Does not account for model complexity: R-squared can increase with additional variables, regardless of their relevance.\\n\\nBy using R-squared, you can gain insights into the performance and explanatory power of your regression model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write Python code to fit a simple linear regression model using scikit-learn and print the slope and intercept.\n",
        "#(Include your Python code and output in the code box below.)\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get the slope and intercept\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "print(f\"Linear Regression Equation: y = {intercept:.2f} + {slope:.2f}x\")\n",
        "\n",
        "print(f\"Slope: {slope:.2f}\")\n",
        "print(f\"Intercept: {intercept:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9KxI8UPi8Cv",
        "outputId": "c5e28172-9933-403d-cadc-23fd1a9f3776"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Equation: y = 6.50 + 1.15x\n",
            "Slope: 1.15\n",
            "Intercept: 6.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. How do you interpret the coefficients in a simple linear regression model?\n",
        "\"\"\" Interpreting Coefficients in Simple Linear Regression\n",
        "In a simple linear regression model, there are two coefficients:\n",
        "1. Intercept (β0): The expected value of the dependent variable (y) when the independent variable (x) is 0.\n",
        "2. Slope (β1): The change in the dependent variable (y) for a one-unit change in the independent variable (x), while holding all other factors constant.\n",
        "\n",
        "Interpretation:\n",
        "- Slope (β1):\n",
        "    - Positive slope: As x increases, y tends to increase.\n",
        "    - Negative slope: As x increases, y tends to decrease.\n",
        "    - Magnitude: The larger the absolute value of the slope, the stronger the relationship between x and y.\n",
        "- Intercept (β0):\n",
        "    - Meaningful intercept: When x = 0 is within the range of the data, the intercept represents the expected value of y.\n",
        "    - Not always meaningful: When x = 0 is outside the range of the data, the intercept may not have a practical interpretation.\n",
        "\n",
        "Example:\n",
        "Suppose we have a simple linear regression model: y = 2 + 3x\n",
        "- Intercept (β0 = 2): The expected value of y is 2 when x is 0.\n",
        "- Slope (β1 = 3): For every one-unit increase in x, y is expected to increase by 3 units.\n",
        "\n",
        "By interpreting the coefficients, you can gain insights into the relationship between the variables and make predictions based on the model.\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "I6bz8hi_g8QC",
        "outputId": "3618efcc-6ac7-4d8b-c191-7d644c226bea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Interpreting Coefficients in Simple Linear Regression\\nIn a simple linear regression model, there are two coefficients:\\n1. Intercept (β0): The expected value of the dependent variable (y) when the independent variable (x) is 0.\\n2. Slope (β1): The change in the dependent variable (y) for a one-unit change in the independent variable (x), while holding all other factors constant.\\n\\nInterpretation:\\n- Slope (β1):\\n    - Positive slope: As x increases, y tends to increase.\\n    - Negative slope: As x increases, y tends to decrease.\\n    - Magnitude: The larger the absolute value of the slope, the stronger the relationship between x and y.\\n- Intercept (β0):\\n    - Meaningful intercept: When x = 0 is within the range of the data, the intercept represents the expected value of y.\\n    - Not always meaningful: When x = 0 is outside the range of the data, the intercept may not have a practical interpretation.\\n\\nExample:\\nSuppose we have a simple linear regression model: y = 2 + 3x\\n- Intercept (β0 = 2): The expected value of y is 2 when x is 0.\\n- Slope (β1 = 3): For every one-unit increase in x, y is expected to increase by 3 units.\\n\\nBy interpreting the coefficients, you can gain insights into the relationship between the variables and make predictions based on the model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}